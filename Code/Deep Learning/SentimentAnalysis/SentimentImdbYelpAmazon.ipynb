{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from review files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewDataPath = {'yelp': 'data/yelp_labelled.txt',\n",
    "                 'amazon': 'data/amazon_cells_labelled.txt',\n",
    "                 'imdb': 'data/imdb_labelled.txt'}\n",
    "reviewList = []\n",
    "\n",
    "for source, filepath in reviewDataPath.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    # Add another column filled with the source name\n",
    "    df['source'] = source \n",
    "    reviewList.append(df)\n",
    "\n",
    "df = pd.concat(reviewList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_imdb = df[df['source'] == 'amazon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      "sentence    1000 non-null object\n",
      "label       1000 non-null int64\n",
      "source      1000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 31.2+ KB\n"
     ]
    }
   ],
   "source": [
    "review_imdb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label source\n",
      "0                           Wow... Loved this place.      1   yelp\n",
      "1                                 Crust is not good.      0   yelp\n",
      "2          Not tasty and the texture was just nasty.      0   yelp\n",
      "3  Stopped by during the late May bank holiday of...      1   yelp\n",
      "4  The selection on the menu was great and so wer...      1   yelp\n",
      "5     Now I am getting angry and I want my damn pho.      0   yelp\n",
      "6              Honeslty it didn't taste THAT fresh.)      0   yelp\n",
      "7  The potatoes were like rubber and you could te...      0   yelp\n",
      "8                          The fries were great too.      1   yelp\n",
      "9                                     A great touch.      1   yelp\n"
     ]
    }
   ],
   "source": [
    "# Just a look at data\n",
    "print(df.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = review_yelp['sentence'].values\n",
    "\n",
    "y = review_yelp['label'].values\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "# The indexing is ordered after the most common words in the text, \n",
    "# which you can see by the word the having the index 1. \n",
    "# It is important to note that the index 0 is reserved \n",
    "# and is not assigned to any word. This zero index is used for padding,\n",
    "# because every statement is not of same size\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry, I will not be getting food from here anytime soon :('\n",
      " 'Of all the dishes, the salmon was the best, but all were great.'\n",
      " 'The fries were not hot, and neither was my burger.'\n",
      " \"In fact I'm going to round up to 4 stars, just because she was so awesome.\"\n",
      " 'Will go back next trip out.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[740, 4, 46, 12, 20, 160, 10, 72, 35, 355, 232]\n",
      "[11, 43, 1, 171, 1, 283, 3, 1, 47, 26, 43, 24, 22]\n",
      "[1, 233, 24, 12, 209, 2, 741, 3, 23, 125]\n",
      "[14, 356, 83, 126, 5, 742, 59, 5, 357, 96, 41, 127, 234, 3, 25, 161]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])\n",
    "print(X_train[2])\n",
    "print(X_train[3])\n",
    "print(X_train[4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With Tokenizer, the resulting vectors equal the length of each text, and the numbers donâ€™t denote counts,\n",
    "but rather correspond to the word values from the dictionary tokenizer.word_index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAD Sequance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Each text sequence has in most cases different length of words. \n",
    "To counter this, pad_sequence() is used ,which simply pads the sequence of words with zeros. \n",
    "By default, it prepends zeros but we want to append them.\n",
    "Typically it does not matter whether you prepend or append zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen parameter to specify how long the sequences should be. \n",
    "#This cuts sequences that exceed that number.\n",
    "\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[740   4  46  12  20 160  10  72  35 355 232   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14 356  83 126   5 742  59   5 357  96  41 127 234   3  25 161   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 50)           87350     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                50010     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 137,371\n",
      "Trainable params: 137,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# vocab size is 1750 \n",
    "# input_length is size of review text after tokenization and pad sequance\n",
    "embedding_dim = 50\n",
    "\n",
    "\n",
    "model.add(layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embedding_dim,\n",
    "                           input_length=maxlen))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples, validate on 250 samples\n",
      "Epoch 1/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.6938 - acc: 0.5013 - val_loss: 0.6953 - val_acc: 0.4800\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 0s 387us/step - loss: 0.6911 - acc: 0.5267 - val_loss: 0.6919 - val_acc: 0.6000\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 0s 427us/step - loss: 0.6754 - acc: 0.5800 - val_loss: 0.6853 - val_acc: 0.5080\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 0s 389us/step - loss: 0.6133 - acc: 0.6720 - val_loss: 0.6692 - val_acc: 0.5560\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 0s 418us/step - loss: 0.5027 - acc: 0.8093 - val_loss: 0.6570 - val_acc: 0.6040\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 0s 415us/step - loss: 0.4138 - acc: 0.9227 - val_loss: 0.6679 - val_acc: 0.6120\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 0s 406us/step - loss: 0.3529 - acc: 0.9667 - val_loss: 0.6388 - val_acc: 0.6840\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 0s 415us/step - loss: 0.3162 - acc: 0.9867 - val_loss: 0.6998 - val_acc: 0.6280\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 0s 414us/step - loss: 0.2871 - acc: 0.9907 - val_loss: 0.6364 - val_acc: 0.6720\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 0s 419us/step - loss: 0.2643 - acc: 0.9933 - val_loss: 0.6570 - val_acc: 0.6800\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 0s 404us/step - loss: 0.2449 - acc: 0.9973 - val_loss: 0.7537 - val_acc: 0.6440\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 0s 425us/step - loss: 0.2289 - acc: 0.9987 - val_loss: 0.7262 - val_acc: 0.6480\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 0s 427us/step - loss: 0.2142 - acc: 1.0000 - val_loss: 0.7370 - val_acc: 0.6480\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 0s 427us/step - loss: 0.2015 - acc: 1.0000 - val_loss: 0.7103 - val_acc: 0.6640\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 0s 427us/step - loss: 0.1902 - acc: 1.0000 - val_loss: 0.7378 - val_acc: 0.6720\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 0s 419us/step - loss: 0.1800 - acc: 1.0000 - val_loss: 0.7399 - val_acc: 0.6680\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 0s 419us/step - loss: 0.1707 - acc: 1.0000 - val_loss: 0.7293 - val_acc: 0.6800\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 0s 427us/step - loss: 0.1616 - acc: 1.0000 - val_loss: 0.7338 - val_acc: 0.6800\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 0s 433us/step - loss: 0.1536 - acc: 1.0000 - val_loss: 0.7728 - val_acc: 0.6680\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 0s 411us/step - loss: 0.1462 - acc: 1.0000 - val_loss: 0.7633 - val_acc: 0.6800\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 10, 46, 191, 76]]\n",
      "[[ 16  10  46 191  76   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "phrase = \"good food ,will come again\"\n",
    "#phrase = \"bad service\"\n",
    "\n",
    "tokens = tokenizer.texts_to_sequences([phrase])\n",
    "pad_tokens = pad_sequences(tokens, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(tokens)\n",
    "print(pad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = model.predict_classes(pad_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSentiments ( indexvalue):\n",
    "    \n",
    "    reviewSentiment = ''\n",
    "    \n",
    "    if (val[0][0] == 0):\n",
    "        reviewSentiment = 'Customer is gone forever,'\n",
    "    else:\n",
    "        reviewSentiment = 'you got back your customer'\n",
    "\n",
    "    return reviewSentiment;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you got back your customer\n"
     ]
    }
   ],
   "source": [
    "print(predictSentiments(val[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model to re-use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5'\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Deletes the existing model\n",
    "#del model  \n",
    "\n",
    "\n",
    "# saving tokenizer \n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer_saved = pickle.load(handle)\n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "model_saved = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90, 19]]\n",
      "[[90 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#review_sen = \"good food ,will come again\"\n",
    "review_sen = \"bad service\"\n",
    "\n",
    "tokens_sen = tokenizer_saved.texts_to_sequences([review_sen])\n",
    "pad_tokens_sen = pad_sequences(tokens_sen, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(tokens_sen)\n",
    "print(pad_tokens_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer is gone forever,\n"
     ]
    }
   ],
   "source": [
    "val = model.predict_classes(pad_tokens_sen)\n",
    "print(predictSentiments(val[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=maxlen))\n",
    "\n",
    "model2.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(X_train, y_train,\n",
    "                    epochs=20,verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model2.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model2.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-Trained GloVe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    \n",
    "    vocab_size = len(word_index) + 1 \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as file:\n",
    "        for line in file:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                print(\"{} {} \".format(word,idx))\n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "filePath = 'GloVe_PreTrained/glove.6B.50d.txt'\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(filePath,\n",
    "                                           tokenizer.word_index, \n",
    "                                           embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(layers.Embedding(vocab_size, \n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)) # Make it False\n",
    "#model3.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model3.add(layers.GlobalMaxPool1D())\n",
    "\n",
    "model3.add(layers.Dense(10, activation='relu'))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(X_train, y_train,\n",
    "                    epochs=20,verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model3.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model3.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
